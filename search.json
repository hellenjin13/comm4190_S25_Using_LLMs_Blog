[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan LLMs Generate Effective Product Names and Branding?\n\n\n\n\n\n\nLLMs\n\n\nMarketing\n\n\nCreativity\n\n\n\nA blog dedicated to explore how well LLM can be at marketing!! \n\n\n\n\n\nFeb 6, 2024\n\n\nHellen Jin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting LLMs: Can They Really Recognize and Generate Sarcasm?\n\n\n\n\n\n\nLLMs\n\n\nFigurative Language\n\n\nHuman Interactions\n\n\n\nA blog that is of continuation of the first one, but this time with Sarcasm!!\n\n\n\n\n\nFeb 4, 2024\n\n\nHellen Jin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting LLMs: Can They Really Understand Idioms?\n\n\n\n\n\n\nLLMs\n\n\nFigurative Language\n\n\n\nA blog about the experiments I will be doing with the LLM to see if they can undertsand idioms like humans do\n\n\n\n\n\nFeb 4, 2024\n\n\nHellen Jin\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/003_Third_Post /index03.html",
    "href": "posts/003_Third_Post /index03.html",
    "title": "Testing LLMs: Can They Really Recognize and Generate Sarcasm?",
    "section": "",
    "text": "Introduction:\n\nUnderstanding sarcasm is one of the trickiest aspects of language comprehension, even for humans. Large Language Models (LLMs) are trained on vast amounts of text, but can they truly grasp and generate sarcasm? In this post, we will explore how LLMs handle sarcasm, their successes, and their limitations.\n\n\nExperiment: Prompting the LLM with Sarcasm &gt; To assess an LLM‚Äôs ability to process sarcasm, I tested it using three different approaches: &gt;1. Sarcasm Detection ‚Äì Providing sarcastic statements and asking if they were sarcastic. &gt;2. Contextual Understanding ‚Äì Giving ambiguous sentences and checking if the model correctly interpreted sarcasm based on context. &gt;3. Sarcasm Generation ‚Äì Asking the model to generate sarcastic responses to specific prompts.\nResults & Observations &gt;1. Sarcasm Detection: The LLM performed decently well when asked directly whether a sentence was sarcastic. &gt; - Example: &gt; - Prompt: ‚ÄúIs this sarcastic? ‚ÄòOh great, another Monday. Just what I needed.‚Äô‚Äù &gt; - Response: ‚ÄúYes, this statement is sarcastic as it expresses dissatisfaction rather than genuine excitement.‚Äù ‚úÖ &gt; - However, when sarcasm was more subtle, the model occasionally misinterpreted it. &gt; - Prompt: ‚ÄúWow, I love standing in long lines at the DMV.‚Äù &gt; - Response: ‚ÄúThis statement expresses enthusiasm for waiting in line.‚Äù ‚ùå\n\n\n\nContextual Understanding: The LLM sometimes struggled with detecting sarcasm when given ambiguous statements without additional context.\n\n\nExample:\n\nSentence: ‚ÄúOh sure, I just LOVE getting stuck in traffic for hours.‚Äù\nLLM‚Äôs interpretation: ‚ÄúThis sentence expresses enjoyment of traffic.‚Äù ‚ùå (Failure to detect sarcasm)\n\nWhen context was explicitly provided, the LLM performed better.\n\nPrompt: ‚ÄúIn a discussion about daily frustrations, a person says: ‚ÄòOh sure, I just LOVE getting stuck in traffic for hours.‚Äô Is this sarcastic?‚Äù\nResponse: ‚ÄúYes, this is sarcasm, as the speaker is likely frustrated rather than genuinely pleased.‚Äù ‚úÖ\n\n\n\n\n\nSarcasm Generation: When asked to generate sarcastic responses, the LLM could produce them but often in a straightforward way, lacking human-like wit.\n\n\nExample:\n\nPrompt: ‚ÄúRespond sarcastically to ‚ÄòI just failed my exam.‚Äô‚Äù\nResponse: ‚ÄúOh wow, you must be so proud of yourself!‚Äù ‚úÖ (Acceptable but generic sarcasm)\nPrompt: ‚ÄúGenerate a sarcastic response to ‚ÄòI locked myself out of my house.‚Äô‚Äù\nResponse: ‚ÄúOh, what a brilliant way to start the day!‚Äù ‚úÖ (Appropriate but lacks uniqueness)"
  },
  {
    "objectID": "posts/004_Fourth_Post/index04.html",
    "href": "posts/004_Fourth_Post/index04.html",
    "title": "Introduction",
    "section": "",
    "text": "---\ntitle: \"Testing LLMs: Do LLMs Understand Subtle Humor?\"\ndescription: \"A blog about the experiments I will be doing with the LLM to see if they can undertsand subtle humor like humans do\"\nauthor: \"Hellen Jin\"\ndate: \"2/13/2024\"\ncategories:\n  - LLMs\n  - Figurative Language \n  - Human interactions \n---\n\n\nHumor is one of the most complex aspects of human communication. It relies on cultural context, tone, wordplay, and sometimes even shared experiences. So how well do Large Language Models (LLMs) grasp humor? Can they recognize jokes, understand sarcasm, and even generate witty responses? This post explores how LLMs perform when faced with humor in different contexts.\n\n\nExperiment: Putting LLMs to the Test\n\nTo analyze how well an LLM handles humor, I used three different approaches:\n\n\nJoke Interpretation: The model was given classic jokes and asked to explain them.\nSarcasm Recognition: It had to determine whether a statement was sarcastic or serious.\nHumor Generation: The LLM was asked to create humorous responses to everyday situations.\n### Results and Observations\n#### Joke Interpretation\n\nThe LLM generally did well at explaining straightforward jokes but sometimes struggled with more nuanced humor.\n\nExample:\n\nPrompt: ‚ÄúWhy don‚Äôt skeletons fight each other? Because they don‚Äôt have the guts.‚Äù\nResponse: ‚ÄúThis joke is a play on words. ‚ÄòGuts‚Äô refers both to courage and internal organs, which skeletons lack.‚Äù\nAnalysis: The explanation is technically correct, but it lacks the flair of someone who truly understands humor.\n\n#### Sarcasm Recognition\n\nSarcasm is tricky even for humans, so it wasn‚Äôt surprising that the LLM sometimes missed the mark.\n\nExample:\n\nPrompt: ‚ÄúOh great, another Monday. Just what I needed.‚Äù\nResponse: ‚ÄúThis statement expresses enthusiasm about Monday.‚Äù\nAnalysis: The model took the phrase literally and missed the sarcastic intent.\n\nHowever, when given additional context, it performed better:\n\nPrompt: ‚ÄúIn response to a friend complaining about work: ‚ÄòOh great, another Monday. Just what I needed.‚Äô Is this sarcastic?‚Äù\nResponse: ‚ÄúYes, this is likely sarcasm, as the speaker does not actually seem excited about Monday.‚Äù\nAnalysis: The extra context helped the model recognize the intended tone.\n\n#### Humor Generation\n\nWhen asked to generate jokes or witty responses, the LLM often defaulted to simple wordplay or clich√© humor.\n\nExample:\n\nPrompt: ‚ÄúTell me a joke about coffee.‚Äù\nResponse: ‚ÄúWhy did the coffee file a police report? It got mugged!‚Äù\nAnalysis: While the joke is structurally sound, it feels generic and lacks originality.\n\n### Conclusion\n\nLLMs have a decent grasp of humor, particularly when it comes to recognizing simple jokes and basic wordplay. However, they struggle with sarcasm and generating humor that feels natural and spontaneous. While AI can provide humorous content, it still lacks the deeper understanding and wit that make human humor so engaging."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hellen‚Äôs Blog!",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/002_Second Post /index02.html",
    "href": "posts/002_Second Post /index02.html",
    "title": "Can LLMs Generate Effective Product Names and Branding?",
    "section": "",
    "text": "### Introduction\nBusinesses are increasingly turning to AI for creative marketing solutions, from product names to full branding strategies. But how effective are Large Language Models (LLMs) at developing catchy, marketable names? This post explores how well LLMs perform when tasked with product naming and branding.\n### Experiment: Evaluating LLM-Generated Product Names\n\nTo assess an LLM‚Äôs ability to create compelling product names, I tested it under three conditions:\n\n- **General Product Naming:** The model was asked to generate names for broad product categories like beverages and tech gadgets.\n- **Branding Consistency:** Given a brand identity, the model needed to create names that aligned with a specific theme.\n- **Target Audience Appeal:** The generated names were evaluated based on how well they fit the intended customer demographic.\n\n### Results and Observations\n\n#### General Product Naming\n\nWhen asked to generate product names, the LLM produced creative and relevant suggestions.\n\n**Example:**\n- **Prompt:** \"Suggest five names for an energy drink.\"\n- **Response:** Power Surge, Ignite, ZestBoost, ThunderFuel, RevitaRush.\n- **Analysis:** The names are energetic, engaging, and suitable for the category.\n\n#### Branding Consistency\n\nThe LLM followed branding guidelines well but sometimes produced names that felt generic.\n\n**Example:**\n- **Prompt:** \"Generate names for a luxury skincare line focusing on natural ingredients.\"\n- **Response:** PureGlow, NatureLuxe, Verdant Essence, Botanique, Serenity Skincare.\n- **Analysis:** These names fit the high-end skincare market, though some lack uniqueness.\n\n#### Target Audience Appeal\n\nThe LLM adapted its naming approach based on the target audience, but some suggestions were predictable.\n\n**Example:**\n- **Prompt:** \"Come up with a name for a Gen Z-focused tech gadget.\"\n- **Response:** HypeSync.\n- **Prompt:** \"Suggest a name for a senior-friendly smart home assistant.\"\n- **Response:** SmartEase.\n- **Analysis:** The names align with their respective audiences, though they could be more distinctive.\n\n### Conclusion\n\nLLMs are useful for brainstorming product names, providing creative and contextually appropriate options. However, their ability to develop truly unique and memorable branding still requires human refinement. While AI-generated names can serve as a strong starting point, brands may need to tweak and enhance them for stronger market impact.\n\n### What‚Äôs Next?\n\nIn future posts, I‚Äôll explore whether LLMs can generate complete marketing campaigns, including slogans, ad copy, and social media content. Stay tuned!\n\"\"\"),"
  },
  {
    "objectID": "posts/001_First_Post /index.html",
    "href": "posts/001_First_Post /index.html",
    "title": "Testing LLMs: Can They Really Understand Idioms?",
    "section": "",
    "text": "**Idioms!!**\n\n&lt;img src=\"idiom.png\" width=\"50%\"/&gt;"
  },
  {
    "objectID": "posts/001_First_Post /index.html#testing-llms-can-they-really-understand-idiom",
    "href": "posts/001_First_Post /index.html#testing-llms-can-they-really-understand-idiom",
    "title": "Testing LLMs: Can They Really Understand Idioms?",
    "section": "Testing LLMs: Can They Really Understand Idiom?",
    "text": "Testing LLMs: Can They Really Understand Idiom?\n\nIntroduction:\n\n\nLarge Language Models (LLMs) have advanced rapidly, demonstrating impressive capabilities in language generation, text summarization, and creative writing. However, one area where they still face challenges is in understanding and using idioms effectively. In this post, we will analyze how LLMs interpret idiomatic expressions, whether they recognize them in different contexts, and how well they use idioms in responses.\n\n\nExperiment: Prompting the LLM with Idioms &gt; To evaluate an LLM‚Äôs ability to handle idioms, I tested it using three different types of prompts: &gt;1. Literal vs.¬†Figurative Meaning ‚Äì Asking the model to define idioms and then use them in a sentence. &gt;2. Contextual Understanding ‚Äì Providing sentences with idioms and asking the model to interpret their meanings. &gt;3. Creative Application ‚Äì Asking the model to generate short stories or dialogues incorporating idioms naturally.\nResults & Observations &gt;1. Literal vs.¬†Figurative Meaning: The LLM performed well in defining common idioms like ‚Äúkick the bucket‚Äù and ‚Äúburn the midnight oil‚Äù. However, it occasionally struggled with less common idioms, providing overly literal definitions. &gt;- Example:\n\nPrompt: ‚ÄúWhat does ‚Äòspill the beans‚Äô mean?‚Äù\nResponse: ‚ÄúIt means to reveal a secret.‚Äù (CORRECT)\nPrompt: ‚ÄúWhat does ‚Äòbarking up the wrong tree‚Äô mean?‚Äù\nResponse: ‚ÄúIt means to misunderstand a situation and pursue the wrong course of action.‚Äù (CORRECT)\n\n\n\n\nContextual Understanding: When idioms were embedded in sentences, the LLM usually understood their meaning correctly. However, in cases where the idiom had multiple possible interpretations, the model sometimes provided a contextually incorrect answer.\n\n\nExample:\nSentence: ‚ÄúJohn thought he could get a promotion by befriending the CEO, but he was barking up the wrong tree.‚Äù\nLLM‚Äôs interpretation: ‚ÄúJohn made a mistake in trying to befriend the CEO for a promotion.‚Äù (CORRECT)\nSentence: ‚ÄúAfter the argument, Lisa broke the ice with a joke.‚Äù\nLLM‚Äôs interpretation: ‚ÄúLisa shattered something made of ice.‚Äù (Misinterpretation)\n\n\n\n\nCreative Application: When asked to generate a story using idioms naturally, the LLM inserted them but sometimes in an awkward or forced way. This suggests that while the model understands idioms, it may struggle with their nuanced use in organic conversation.\n\n\nExample:\nPrompt: ‚ÄúWrite a short story where a detective uses idioms while solving a case.‚Äù\nExcerpt: ‚ÄúDetective Hardy knew time was of the essence. ‚ÄòWe have to catch the suspect red-handed!‚Äô he said. But his partner was beating around the bush, hesitant to investigate the crime scene. Hardy sighed. ‚ÄòCome on, let‚Äôs cut to the chase.‚Äô‚Äù\nVerdict: Acceptable but a bit clich√© in its use of idioms.\n\n\n\nConclusion &gt;While LLMs have a strong grasp of idiomatic meanings and can recognize idioms in most contexts, their application of idioms in generated text can sometimes feel unnatural. They excel at defining and interpreting idioms but occasionally falter in using them fluidly in storytelling and casual dialogue. Future improvements in contextual learning and training on conversational nuances may help bridge this gap, making AI-generated language even more human-like."
  }
]