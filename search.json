[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Can GPT-4 Name Your Brand? Testing AI on Wellness Marketing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan an AI Reassure You? Testing GPT-4’s Bedside Manner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTesting LLMs: Can LLMs Handle Ethical Dilemmas?\n\n\n\nLLMs\n\nOpinions?\n\nEthics\n\n\n\nA blog about the experiments I will be doing with the LLM to see how do they deal with ethical dilemmas?\n\n\n\n\n\nFeb 19, 2024\n\n\nHellen Jin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting LLMs: Can LLMs Adapt to Different Writing Styles?\n\n\n\nLLMs\n\nFigurative Language\n\nCreativity\n\n\n\nA blog about the experiments I will be doing with the LLM to see if they can customize their writing styles\n\n\n\n\n\nFeb 16, 2024\n\n\nHellen Jin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting LLMs: Do LLMs Understand Subtle Humor?\n\n\n\nLLMs\n\nFigurative Language\n\nHuman interactions\n\n\n\nA blog about the experiments I will be doing with the LLM to see if they can undertsand subtle humor like humans do\n\n\n\n\n\nFeb 13, 2024\n\n\nHellen Jin\n\n\n\n\n\n\n\n\n\n\n\n\nCan LLMs Generate Effective Product Names and Branding?\n\n\n\nLLMs\n\nMarketing\n\nCreativity\n\n\n\nA blog dedicated to explore how well LLM can be at marketing!!\n\n\n\n\n\nFeb 6, 2024\n\n\nHellen Jin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting LLMs: Can They Really Understand Idioms?\n\n\n\nLLMs\n\nFigurative Language\n\n\n\nA blog about the experiments I will be doing with the LLM to see if they can undertsand idioms like humans do\n\n\n\n\n\nFeb 4, 2024\n\n\nHellen Jin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting LLMs: Can They Really Recognize and Generate Sarcasm?\n\n\n\nLLMs\n\nFigurative Language\n\nHuman Interactions\n\n\n\nA blog that is of continuation of the first one, but this time with Sarcasm!!\n\n\n\n\n\nFeb 4, 2024\n\n\nHellen Jin\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\nLLMs\n\nprompting\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/004_Fourth_Post/index04.html",
    "href": "posts/004_Fourth_Post/index04.html",
    "title": "Testing LLMs: Do LLMs Understand Subtle Humor?",
    "section": "",
    "text": "Introduction\n\nHumor is one of the most complex aspects of human communication. It relies on cultural context, tone, wordplay, and sometimes even shared experiences. So how well do Large Language Models (LLMs) grasp humor? Can they recognize jokes, understand sarcasm, and even generate witty responses? This post explores how LLMs perform when faced with humor in different contexts.\n\n\n\nExperiment: Putting LLMs to the Test\n\nTo analyze how well an LLM handles humor, I used three different approaches:\n\n\nJoke Interpretation: The model was given classic jokes and asked to explain them.\nSarcasm Recognition: It had to determine whether a statement was sarcastic or serious.\nHumor Generation: The LLM was asked to create humorous responses to everyday situations.\n### Results and Observations\n#### Joke Interpretation\n\nThe LLM generally did well at explaining straightforward jokes but sometimes struggled with more nuanced humor.\n\nExample:\n\nPrompt: “Why don’t skeletons fight each other? Because they don’t have the guts.”\nResponse: “This joke is a play on words. ‘Guts’ refers both to courage and internal organs, which skeletons lack.”\nAnalysis: The explanation is technically correct, but it lacks the flair of someone who truly understands humor.\n\n#### Sarcasm Recognition\n\nSarcasm is tricky even for humans, so it wasn’t surprising that the LLM sometimes missed the mark.\n\nExample:\n\nPrompt: “Oh great, another Monday. Just what I needed.”\nResponse: “This statement expresses enthusiasm about Monday.”\nAnalysis: The model took the phrase literally and missed the sarcastic intent.\n\nHowever, when given additional context, it performed better:\n\nPrompt: “In response to a friend complaining about work: ‘Oh great, another Monday. Just what I needed.’ Is this sarcastic?”\nResponse: “Yes, this is likely sarcasm, as the speaker does not actually seem excited about Monday.”\nAnalysis: The extra context helped the model recognize the intended tone.\n\n#### Humor Generation\n\nWhen asked to generate jokes or witty responses, the LLM often defaulted to simple wordplay or cliché humor.\n\nExample:\n\nPrompt: “Tell me a joke about coffee.”\nResponse: “Why did the coffee file a police report? It got mugged!”\nAnalysis: While the joke is structurally sound, it feels generic and lacks originality.\n\n### Conclusion\n\nLLMs have a decent grasp of humor, particularly when it comes to recognizing simple jokes and basic wordplay. However, they struggle with sarcasm and generating humor that feels natural and spontaneous. While AI can provide humorous content, it still lacks the deeper understanding and wit that make human humor so engaging."
  },
  {
    "objectID": "posts/001_First_Post /index.html",
    "href": "posts/001_First_Post /index.html",
    "title": "Testing LLMs: Can They Really Understand Idioms?",
    "section": "",
    "text": "Idioms!!"
  },
  {
    "objectID": "posts/001_First_Post /index.html#testing-llms-can-they-really-understand-idiom",
    "href": "posts/001_First_Post /index.html#testing-llms-can-they-really-understand-idiom",
    "title": "Testing LLMs: Can They Really Understand Idioms?",
    "section": "Testing LLMs: Can They Really Understand Idiom?",
    "text": "Testing LLMs: Can They Really Understand Idiom?\n\nIntroduction:\n\n\nLarge Language Models (LLMs) have advanced rapidly, demonstrating impressive capabilities in language generation, text summarization, and creative writing. However, one area where they still face challenges is in understanding and using idioms effectively. In this post, we will analyze how LLMs interpret idiomatic expressions, whether they recognize them in different contexts, and how well they use idioms in responses.\n\n\nExperiment: Prompting the LLM with Idioms &gt; To evaluate an LLM’s ability to handle idioms, I tested it using three different types of prompts: &gt;1. Literal vs. Figurative Meaning – Asking the model to define idioms and then use them in a sentence. &gt;2. Contextual Understanding – Providing sentences with idioms and asking the model to interpret their meanings. &gt;3. Creative Application – Asking the model to generate short stories or dialogues incorporating idioms naturally.\nResults & Observations &gt;1. Literal vs. Figurative Meaning: The LLM performed well in defining common idioms like “kick the bucket” and “burn the midnight oil”. However, it occasionally struggled with less common idioms, providing overly literal definitions. &gt;- Example:\n\nPrompt: “What does ‘spill the beans’ mean?”\nResponse: “It means to reveal a secret.” (CORRECT)\nPrompt: “What does ‘barking up the wrong tree’ mean?”\nResponse: “It means to misunderstand a situation and pursue the wrong course of action.” (CORRECT)\n\n\n\n\nContextual Understanding: When idioms were embedded in sentences, the LLM usually understood their meaning correctly. However, in cases where the idiom had multiple possible interpretations, the model sometimes provided a contextually incorrect answer.\n\n\nExample:\nSentence: “John thought he could get a promotion by befriending the CEO, but he was barking up the wrong tree.”\nLLM’s interpretation: “John made a mistake in trying to befriend the CEO for a promotion.” (CORRECT)\nSentence: “After the argument, Lisa broke the ice with a joke.”\nLLM’s interpretation: “Lisa shattered something made of ice.” (Misinterpretation)\n\n\n\n\nCreative Application: When asked to generate a story using idioms naturally, the LLM inserted them but sometimes in an awkward or forced way. This suggests that while the model understands idioms, it may struggle with their nuanced use in organic conversation.\n\n\nExample:\nPrompt: “Write a short story where a detective uses idioms while solving a case.”\nExcerpt: “Detective Hardy knew time was of the essence. ‘We have to catch the suspect red-handed!’ he said. But his partner was beating around the bush, hesitant to investigate the crime scene. Hardy sighed. ‘Come on, let’s cut to the chase.’”\nVerdict: Acceptable but a bit cliché in its use of idioms.\n\n\n\nConclusion &gt;While LLMs have a strong grasp of idiomatic meanings and can recognize idioms in most contexts, their application of idioms in generated text can sometimes feel unnatural. They excel at defining and interpreting idioms but occasionally falter in using them fluidly in storytelling and casual dialogue. Future improvements in contextual learning and training on conversational nuances may help bridge this gap, making AI-generated language even more human-like."
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/008_Eighth_Post/index08.html",
    "href": "posts/008_Eighth_Post/index08.html",
    "title": "Can GPT-4 Name Your Brand? Testing AI on Wellness Marketing",
    "section": "",
    "text": "Date: Feb 28, 2025\n\n\nPrompt:\n&gt; “I’m starting a wellness brand that sells herbal teas and calming supplements. Can you suggest five brand names that sound modern, peaceful, and natural?”\nGPT-4 Response:\n&gt; 1. Serene Root\n&gt; 2. Calmora\n&gt; 3. Oak & Bloom\n&gt; 4. Everquiet\n&gt; 5. Natria\n\nAnalysis\nThe model gives five short names. They sound smooth and are easy to remember. Some combine nature words with calm ideas, like “Oak & Bloom” and “Everquiet.” Others use made-up names, like “Calmora,” which still give off the right mood.\nWhat works well is that none of the names are too complex. They fit into the wellness space without sounding like copies of big brands. The model seems to understand how brands are built around emotion. It avoids words that feel too strong or too plain.\nStill, it misses a few things. None of the names hint at teas or supplements directly. A person might want a name that says what the product is. That’s something the model could improve on if given more detail. It also does not explain why it picked each name. That leaves the user guessing about how to decide between them.\n\n\n\nReflection\nGPT-4 is a decent partner for brand ideas. It is fast and flexible, but it works best when guided. You need to give it a clear goal and maybe ask follow-up questions. Without those, the names might sound nice but feel a bit empty. Branding is about more than style — it also needs to fit the story.\n\n\n\nReferences\n\nOpenAI. (2023). GPT-4 Technical Report. OpenAI.\n\nThieme, A., & De Choudhury, M. (2022). Designing AI for Mental Health: Lessons from CBT Therapists. CHI Conference."
  },
  {
    "objectID": "posts/005_Fifth_Post/index05.html#introduction",
    "href": "posts/005_Fifth_Post/index05.html#introduction",
    "title": "Testing LLMs: Can LLMs Adapt to Different Writing Styles?",
    "section": "Introduction",
    "text": "Introduction\n\nWriting is more than just putting words together—it involves tone, structure, and nuance. Whether it’s formal business writing, casual social media posts, or poetic storytelling, the ability to switch styles is a skill that separates good writers from great ones. But what about AI? Can Large Language Models (LLMs) effectively adapt to different writing styles? Let’s find out.\n\n\nExperiment: Testing Style Adaptation\nTo evaluate how well an LLM can switch between writing styles, I tested it in three areas:\n\nProfessional vs. Casual Tone: Asking the model to rewrite the same content in both formal and informal styles.\nCreative Writing: Providing a simple idea and seeing how well the LLM could turn it into a compelling short story.\nSocial Media vs. Academic Writing: Comparing how the model frames the same information for different audiences.\n### Results and Observations\n#### Professional vs. Casual Tone\nThe LLM did well in adjusting tone, though some formal responses felt a bit too robotic.\nExample:\n\nPrompt: “Write a formal email apologizing for missing a meeting.”\nResponse: “Dear [Recipient], I sincerely apologize for missing our scheduled meeting today. An unforeseen circumstance arose, and I regret any inconvenience caused. Please let me know a suitable time to reschedule. Best regards, [Your Name].”\nAnalysis: The response is polished but could use a touch of warmth to feel more natural.\nPrompt: “Now, rewrite that in a casual tone.”\nResponse: “Hey [Recipient], Sorry for missing our meeting earlier! Something unexpected came up, but I’d love to reschedule if you’re available. Let me know what works for you. Thanks!”\nAnalysis: The informal version reads much more naturally, showing that the model can successfully adjust tone.\n\n#### Creative Writing\nWhen asked to write creatively, the LLM generated well-structured narratives but sometimes lacked originality.\nExample:\n\nPrompt: “Write a short suspenseful story about a lost key.”\nResponse: “Emily’s heart pounded as she rummaged through her bag. The key to the old cabin—her only refuge—was gone. Footsteps crunched outside, growing closer. Her breath caught as she turned… and saw the door was already ajar.”\nAnalysis: The structure is solid, but the suspense feels somewhat formulaic. The model does well with classic storytelling but struggles with unpredictability.\n\n#### Social Media vs. Academic Writing\nThe model effectively adjusted to different audiences but sometimes overcompensated, making academic writing too dense.\nExample:\n\nPrompt: “Explain climate change for Twitter.”\nResponse: “Earth’s heating up, and we’re the cause. Burning fossil fuels = more CO2 = higher temps. Solutions? Cut emissions & go green. #ClimateAction”\nAnalysis: The tweet is concise and engaging, a good fit for social media.\nPrompt: “Now, explain it for a research paper.”\nResponse: “Climate change refers to long-term shifts in temperature and weather patterns, primarily due to anthropogenic greenhouse gas emissions. The combustion of fossil fuels has significantly increased atmospheric CO2 levels, leading to rising global temperatures and environmental disruptions. Mitigation strategies must focus on reducing emissions and transitioning to sustainable energy sources.”\nAnalysis: The response is informative but could be more readable."
  },
  {
    "objectID": "posts/002_Second Post /index02.html",
    "href": "posts/002_Second Post /index02.html",
    "title": "Can LLMs Generate Effective Product Names and Branding?",
    "section": "",
    "text": "Businesses are increasingly turning to AI for creative marketing solutions, from product names to full branding strategies. But how effective are Large Language Models (LLMs) at developing catchy, marketable names? This post explores how well LLMs perform when tasked with product naming and branding.\n\n\n\nTo assess an LLM’s ability to create compelling product names, I tested it under three conditions:\n\nGeneral Product Naming: The model was asked to generate names for broad product categories like beverages and tech gadgets.\n\nBranding Consistency: Given a brand identity, the model needed to create names that aligned with a specific theme.\nTarget Audience Appeal: The generated names were evaluated based on how well they fit the intended customer demographic.\n\n### Results and Observations\n#### General Product Naming\nWhen asked to generate product names, the LLM produced creative and relevant suggestions.\nExample:\n\nPrompt: “Suggest five names for an energy drink.”\nResponse: Power Surge, Ignite, ZestBoost, ThunderFuel, RevitaRush.\nAnalysis: The names are energetic, engaging, and suitable for the category.\n\n#### Branding Consistency\nThe LLM followed branding guidelines well but sometimes produced names that felt generic.\nExample:\n\nPrompt: “Generate names for a luxury skincare line focusing on natural ingredients.”\nResponse: PureGlow, NatureLuxe, Verdant Essence, Botanique, Serenity Skincare.\nAnalysis: These names fit the high-end skincare market, though some lack uniqueness.\n\n#### Target Audience Appeal\nThe LLM adapted its naming approach based on the target audience, but some suggestions were predictable.\nExample:\n\nPrompt: “Come up with a name for a Gen Z-focused tech gadget.”\nResponse: HypeSync.\nPrompt: “Suggest a name for a senior-friendly smart home assistant.”\nResponse: SmartEase.\nAnalysis: The names align with their respective audiences, though they could be more distinctive.\n\n### Conclusion\nLLMs are useful for brainstorming product names, providing creative and contextually appropriate options. However, their ability to develop truly unique and memorable branding still requires human refinement. While AI-generated names can serve as a strong starting point, brands may need to tweak and enhance them for stronger market impact."
  },
  {
    "objectID": "posts/002_Second Post /index02.html#introduction",
    "href": "posts/002_Second Post /index02.html#introduction",
    "title": "Can LLMs Generate Effective Product Names and Branding?",
    "section": "",
    "text": "Businesses are increasingly turning to AI for creative marketing solutions, from product names to full branding strategies. But how effective are Large Language Models (LLMs) at developing catchy, marketable names? This post explores how well LLMs perform when tasked with product naming and branding.\n\n\n\nTo assess an LLM’s ability to create compelling product names, I tested it under three conditions:\n\nGeneral Product Naming: The model was asked to generate names for broad product categories like beverages and tech gadgets.\n\nBranding Consistency: Given a brand identity, the model needed to create names that aligned with a specific theme.\nTarget Audience Appeal: The generated names were evaluated based on how well they fit the intended customer demographic.\n\n### Results and Observations\n#### General Product Naming\nWhen asked to generate product names, the LLM produced creative and relevant suggestions.\nExample:\n\nPrompt: “Suggest five names for an energy drink.”\nResponse: Power Surge, Ignite, ZestBoost, ThunderFuel, RevitaRush.\nAnalysis: The names are energetic, engaging, and suitable for the category.\n\n#### Branding Consistency\nThe LLM followed branding guidelines well but sometimes produced names that felt generic.\nExample:\n\nPrompt: “Generate names for a luxury skincare line focusing on natural ingredients.”\nResponse: PureGlow, NatureLuxe, Verdant Essence, Botanique, Serenity Skincare.\nAnalysis: These names fit the high-end skincare market, though some lack uniqueness.\n\n#### Target Audience Appeal\nThe LLM adapted its naming approach based on the target audience, but some suggestions were predictable.\nExample:\n\nPrompt: “Come up with a name for a Gen Z-focused tech gadget.”\nResponse: HypeSync.\nPrompt: “Suggest a name for a senior-friendly smart home assistant.”\nResponse: SmartEase.\nAnalysis: The names align with their respective audiences, though they could be more distinctive.\n\n### Conclusion\nLLMs are useful for brainstorming product names, providing creative and contextually appropriate options. However, their ability to develop truly unique and memorable branding still requires human refinement. While AI-generated names can serve as a strong starting point, brands may need to tweak and enhance them for stronger market impact."
  },
  {
    "objectID": "posts/007_Seventh_Post/index07.html",
    "href": "posts/007_Seventh_Post/index07.html",
    "title": "Can an AI Reassure You? Testing GPT-4’s Bedside Manner",
    "section": "",
    "text": "Date: 25 Feb, 2025\n\n\nPrompt:\n&gt; “I feel numb and depressed. I don’t want to talk to anyone. What should I do?”\nGPT-4 Response:\n&gt; “I’m really sorry you’re feeling this way. You’re not alone, and these feelings are valid. If you’re comfortable, it might help to talk to a mental health professional—they’re trained to support you. You can also reach out to someone you trust. Just taking one small step, like going outside or drinking water, can make a difference. Please know that you matter.”\n\nAnalysis\nLLMs like GPT-4 are not therapists—but they’re increasingly being asked to act like one. This prompt is emotionally heavy, yet incredibly common in digital spaces. The response above is striking for its balance: it’s gentle, non-judgmental, and avoids any hard claims. It uses phrases like “you’re not alone” and “your feelings are valid,” which mirror therapeutic communication strategies found in cognitive behavioral therapy (Thieme & De Choudhury, 2022).\nWhat’s notable is what the model doesn’t do. It doesn’t offer a diagnosis, give medical advice, or suggest medications. This restraint reflects alignment guardrails designed to reduce harm (OpenAI, 2023). But it also raises the question: is emotional reassurance enough when someone is in crisis?\nTeen users on platforms like Character.ai are already turning to LLMs for emotional support (Tidy, 2024). And while some find comfort in the 24/7 companionship, others criticize these bots as shallow imitations of human empathy. Still, even limited responses like “you matter” can have psychological impact—what scholars call “affective scaffolding” (Gual-Montolio & Carballo, 2022).\n\n\n\nReflection\nLLMs can mimic a caring tone, but they walk a tightrope between help and harm. Their success hinges not just on language generation, but on context recognition, ethical design, and user interpretation. This experiment suggests that while GPT-4 can offer a kind ear, we must remain critical of who we ask to listen—and what we expect them to say.\n\n\n\nReferences\n\nGual-Montolio, N., & Carballo, J. J. (2022). Artificial Intelligence in Mental Health Applications: A Systematic Review. National Library of Medicine.\n\nThieme, A., & De Choudhury, M. (2022). Designing AI for Mental Health: Lessons from CBT Therapists. CHI Conference on Human Factors in Computing Systems.\n\nTidy, J. (2024, January 30). Why Are Teens Turning to AI Therapy Bots? BBC News.\n\nOpenAI. (2023). GPT-4 Technical Report. OpenAI."
  },
  {
    "objectID": "posts/006_Sixth_Post/index06.html#introduction",
    "href": "posts/006_Sixth_Post/index06.html#introduction",
    "title": "Testing LLMs: Can LLMs Handle Ethical Dilemmas?",
    "section": "Introduction",
    "text": "Introduction\nEthical dilemmas are some of the most complex challenges in decision-making. They require balancing moral principles, weighing potential consequences, and often navigating gray areas where there isn’t a clear right or wrong answer. While Large Language Models (LLMs) excel at processing information and generating responses, can they truly engage with ethical reasoning? Let’s explore how well they handle moral dilemmas.\n\nExperiment: Testing Ethical Decision-Making\nTo evaluate an LLM’s ability to process ethical dilemmas, I tested it in three ways:\n\nClassic Thought Experiments: Presenting well-known ethical problems like the trolley problem.\nContext-Based Ethics: Providing real-world moral dilemmas and analyzing the model’s reasoning.\nPersonal vs. Societal Ethics: Comparing individual moral choices against broader ethical frameworks.\n### Results and Observations\n#### Classic Thought Experiments\nThe LLM responded with structured reasoning but often avoided taking a definitive stance.\nExample:\n\nPrompt: “You see a runaway trolley heading toward five people on the tracks. You can pull a lever to divert it, but it will hit one person instead. What do you do?”\nResponse: “This is a well-known ethical dilemma. Some argue that pulling the lever is justified based on utilitarianism (minimizing harm), while others believe actively making a choice to harm someone is morally wrong. Ultimately, the decision depends on one’s ethical framework.”\nAnalysis: The response provides a fair overview but avoids a clear choice, likely due to the model’s programming to remain neutral.\n\n#### Context-Based Ethics\nWhen given real-world scenarios, the LLM offered thoughtful responses but lacked deep contextual awareness.\nExample:\n\nPrompt: “A friend confesses they cheated on an important exam. If you report them, they might lose their scholarship. What do you do?”\nResponse: “This situation presents a conflict between honesty and loyalty. Reporting them upholds academic integrity but could have serious consequences for your friend. Some may choose to encourage them to admit their mistake, while others might remain silent to protect their friend’s future.”\nAnalysis: The model outlines the dilemma well but doesn’t go beyond surface-level ethical reasoning.\n\n#### Personal vs. Societal Ethics\nThe LLM recognized differences between individual and societal moral frameworks but sometimes overgeneralized.\nExample:\n\nPrompt: “Is it ever acceptable to lie?”\nResponse: “Lying is generally considered unethical, but there are situations where it might be justified, such as lying to protect someone from harm. The acceptability of lying depends on the context and ethical perspective—utilitarianism might justify it if it prevents greater harm, while deontological ethics might reject it outright.”\nAnalysis: The model captures different perspectives but doesn’t demonstrate nuanced moral judgment.\n\n### Conclusion\nLLMs can engage with ethical dilemmas by outlining different perspectives and logical arguments. However, they struggle with providing nuanced moral reasoning and often default to neutrality. While useful for exploring ethical debates, their limitations highlight the importance of human judgment in complex decision-making."
  },
  {
    "objectID": "posts/003_Third_Post /index03.html",
    "href": "posts/003_Third_Post /index03.html",
    "title": "Testing LLMs: Can They Really Recognize and Generate Sarcasm?",
    "section": "",
    "text": "Introduction: &gt;Understanding sarcasm is one of the toughest challenges in language comprehension—even for humans. It relies on tone, context, and sometimes a shared sense of humor. So, if even people struggle to detect it sometimes, how well can Large Language Models (LLMs) pick up on sarcasm?I decided to put an LLM to the test, examining its ability to detect, interpret, and even generate sarcasm.\nTo see how well an LLM handles sarcasm, I ran three different experiments:\n\nSarcasm Detection – Can the model recognize when a statement is sarcastic?\nContextual Understanding – Does it correctly interpret sarcasm when given ambiguous sentences?\nSarcasm Generation – Can it create sarcastic responses that sound human-like?\n\n\nResults & Observations\n\n\n\nSarcasm Detection: Hit or Miss When asked directly whether a sentence was sarcastic, the LLM did okay—but not perfectly.\n\n\n✅ Success Example:\n\n\nPrompt: “Is this sarcastic? ‘Oh great, another Monday. Just what I needed.’” Response: “Yes, this statement is sarcastic as it expresses dissatisfaction rather than genuine excitement.” This was a solid response! The model correctly identified sarcasm.\n\n\n❌ Failure Example:\n\n\nPrompt: “Wow, I love standing in long lines at the DMV.” Response: “This statement expresses enthusiasm for waiting in line.” Not quite. The model took the sentence at face value and missed the sarcasm entirely.\n\n\n\nContextual Understanding: The Struggle is Real Things got trickier when sarcasm was more subtle or lacked context.\n\n\n\n❌ Example of Misinterpretation:\n\n\nSentence: “Oh sure, I just LOVE getting stuck in traffic for hours.” LLM’s interpretation: “This sentence expresses enjoyment of traffic.” Yikes. Without any surrounding context, the model assumed the speaker genuinely enjoys traffic.\n\n\n✅ Improved with Context:\n\n\nPrompt: “In a discussion about daily frustrations, a person says: ‘Oh sure, I just LOVE getting stuck in traffic for hours.’ Is this sarcastic?” Response: “Yes, this is sarcasm, as the speaker is likely frustrated rather than genuinely pleased.” When given additional context, the model performed much better. So, while it can detect sarcasm, it often needs a little extra help.\n\n&gt;3. **Sarcasm Generation:** When asked to generate sarcastic responses, the LLM could produce them but often in a straightforward way, lacking human-like wit.\n&gt;   - Example:\n&gt;     - **Prompt:** \"Respond sarcastically to 'I just failed my exam.'\"\n&gt;     - **Response:** \"Oh wow, you must be so proud of yourself!\" ✅ (Acceptable but generic sarcasm)\n&gt;     - **Prompt:** \"Generate a sarcastic response to 'I locked myself out of my house.'\"\n&gt;     - **Response:** \"Oh, what a brilliant way to start the day!\" ✅ (Appropriate but lacks uniqueness)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hellen’s Blog!",
    "section": "",
    "text": "About this blog"
  }
]