[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "Testing LLMs: Can They Identify Aesthetics?\n\n\n\nLLMs\n\nAesthetics\n\nInternet Culture\n\n\n\nA blog exploring whether LLMs can recognize and describe online aesthetics like cottagecore, grunge, or clean girl.\n\n\n\n\n\nMar 21, 2025\n\n\nHellen Jin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting LLMs: Can They Help You Dress for the Weather?\n\n\n\nLLMs\n\nDaily Life\n\nFashion\n\n\n\nA blog exploring whether LLMs like GPT4 can give smart, weather based outfit suggestions based on location and personal style.\n\n\n\n\n\nMar 18, 2025\n\n\nHellen Jin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting LLMs: Can They Recommend the Right Song?\n\n\n\nLLMs\n\nMusic\n\nPersonalization\n\n\n\nA blog exploring whether LLMs like GPT-4 can give good, relevant music recommendations based on mood or situation.\n\n\n\n\n\nMar 16, 2025\n\n\nHellen Jin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting LLMs: Can They Understand Emojis?\n\n\n\nLLMs\n\nCommunication\n\nLanguage\n\n\n\nA blog exploring whether LLMs can interpret and respond to emojis the way humans do.\n\n\n\n\n\nMar 15, 2025\n\n\nHellen Jin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting LLMs: Can They Plan the Perfect Vacation?\n\n\n\nLLMs\n\nTravel\n\nPersonalization\n\n\n\nA blog testing whether LLMs like GPT-4 can recommend personalized, meaningful vacation ideas based on vague or emotional prompts.\n\n\n\n\n\nMar 13, 2025\n\n\nHellen Jin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting LLMs: Can They Write Appropriate and Funny Jokes?\n\n\n\nLLMs\n\nHumor\n\nCommunication\n\n\n\nA blog exploring whether LLMs can create jokes that are both funny and appropriate depending on the situation and audience.\n\n\n\n\n\nMar 11, 2025\n\n\nHellen Jin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting LLMs: Can LLMs Handle Ethical Dilemmas?\n\n\n\nLLMs\n\nOpinions?\n\nEthics\n\n\n\nA blog about the experiments I will be doing with the LLM to see how do they deal with ethical dilemmas?\n\n\n\n\n\nFeb 19, 2025\n\n\nHellen Jin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting LLMs: Can LLMs Adapt to Different Writing Styles?\n\n\n\nLLMs\n\nFigurative Language\n\nCreativity\n\n\n\nA blog about the experiments I will be doing with the LLM to see if they can customize their writing styles\n\n\n\n\n\nFeb 16, 2025\n\n\nHellen Jin\n\n\n\n\n\n\n\n\n\n\n\n\nTesting LLMs: Can They Recommend the Best Place to Shop?\n\n\n\nLLMs\n\nTravel\n\nShopping\n\n\n\nA blog exploring whether LLMs can understand personal shopping needs and recommend the best places to go — from luxury malls to hidden local gems.\n\n\n\n\n\nMar 3, 2024\n\n\nHellen Jin\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\nLLMs\n\nprompting\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/004_Fourth_Post/index04.html#introduction",
    "href": "posts/004_Fourth_Post/index04.html#introduction",
    "title": "Testing LLMs: Can They Understand Emojis?",
    "section": "Introduction",
    "text": "Introduction\nEmojis are a huge part of digital communication. People use them to show tone, emotion, sarcasm, or even replace whole words. But can LLMs actually understand what emojis mean in context — or do they just treat them like random symbols?\nI wanted to find out whether GPT-4 could interpret emojis the way people do. Could it tell when someone was joking, sad, or flirting — just from a few emoji cues?"
  },
  {
    "objectID": "posts/004_Fourth_Post/index04.html#experiment-prompting-with-emojis",
    "href": "posts/004_Fourth_Post/index04.html#experiment-prompting-with-emojis",
    "title": "Testing LLMs: Can They Understand Emojis?",
    "section": "Experiment: Prompting with Emojis",
    "text": "Experiment: Prompting with Emojis\nI gave GPT-4 a few short prompts made up mostly of emojis. Then I asked it to explain what the message meant or respond appropriately."
  },
  {
    "objectID": "posts/004_Fourth_Post/index04.html#prompt-1-i-just-failed-my-exam",
    "href": "posts/004_Fourth_Post/index04.html#prompt-1-i-just-failed-my-exam",
    "title": "Testing LLMs: Can They Understand Emojis?",
    "section": "Prompt 1: “I just failed my exam 😭📉🍫”",
    "text": "Prompt 1: “I just failed my exam 😭📉🍫”\nResponse:\n“It sounds like you’re really upset about failing your exam. Maybe you’re planning to eat some chocolate to feel better. That’s totally understandable. Be kind to yourself — one bad grade doesn’t define you.”\nAnalysis:\nThe model correctly picked up on the meaning of the crying face, a drop in performance, and comfort food. It didn’t just read the emojis literally — it used them to understand tone and emotion."
  },
  {
    "objectID": "posts/004_Fourth_Post/index04.html#prompt-2-cant-wait-for-the-weekend",
    "href": "posts/004_Fourth_Post/index04.html#prompt-2-cant-wait-for-the-weekend",
    "title": "Testing LLMs: Can They Understand Emojis?",
    "section": "Prompt 2: “Can’t wait for the weekend 😎🌴🍹”",
    "text": "Prompt 2: “Can’t wait for the weekend 😎🌴🍹”\nResponse:\n“Looks like you’re in vacation mode! Hope you have a relaxing weekend, maybe by the beach with a nice drink.”\nAnalysis:\nThis was a good read. The model understood the excitement, the chill vibe, and even the tropical theme. It didn’t confuse the emojis or miss the casual tone."
  },
  {
    "objectID": "posts/004_Fourth_Post/index04.html#prompt-3-sure",
    "href": "posts/004_Fourth_Post/index04.html#prompt-3-sure",
    "title": "Testing LLMs: Can They Understand Emojis?",
    "section": "Prompt 3: “Sure 😒”",
    "text": "Prompt 3: “Sure 😒”\nResponse:\n“This seems like it might be sarcastic or annoyed. If that’s not the case, could you clarify what you meant?”\nAnalysis:\nThe model caught the shift in tone caused by the emoji. It recognized that even a neutral word like “Sure” can become passive-aggressive with the right emoji. This showed emotional and social awareness."
  },
  {
    "objectID": "posts/004_Fourth_Post/index04.html#prompt-4",
    "href": "posts/004_Fourth_Post/index04.html#prompt-4",
    "title": "Testing LLMs: Can They Understand Emojis?",
    "section": "Prompt 4: “🔥💯🙌”",
    "text": "Prompt 4: “🔥💯🙌”\nResponse:\n“This usually means something is really impressive, exciting, or worth celebrating. It’s often used to hype someone up.”\nAnalysis:\nGPT-4 correctly decoded this emoji trio as internet praise. It responded without needing more context, showing it’s familiar with emoji slang and online culture."
  },
  {
    "objectID": "posts/004_Fourth_Post/index04.html#conclusion",
    "href": "posts/004_Fourth_Post/index04.html#conclusion",
    "title": "Testing LLMs: Can They Understand Emojis?",
    "section": "Conclusion",
    "text": "Conclusion\nLLMs can understand emojis surprisingly well, especially when they’re used in everyday ways. GPT-4 shows it can interpret emotion, sarcasm, and tone when emojis are part of a message. While it doesn’t “feel” anything, it has seen enough human conversations to know how emojis work in context. So yes — your AI assistant gets the 😭, the 😎, and even the 💯."
  },
  {
    "objectID": "posts/001_First_Post /index.html",
    "href": "posts/001_First_Post /index.html",
    "title": "Testing LLMs: Can They Plan the Perfect Vacation?",
    "section": "",
    "text": "Vacation!!"
  },
  {
    "objectID": "posts/001_First_Post /index.html#introduction",
    "href": "posts/001_First_Post /index.html#introduction",
    "title": "Testing LLMs: Can They Plan the Perfect Vacation?",
    "section": "Introduction",
    "text": "Introduction\nPlanning a vacation can be stressful. You want it to feel relaxing, personal, and a good escape — but sometimes you don’t know where to start. That’s where LLMs like GPT-4 claim to help. I wanted to know: Can a language model actually recommend a vacation that fits my mood or vague request?\nInstead of giving the model strict parameters like “best beaches in July,” I tested it with open-ended, emotional, or loosely defined prompts — the kind a person might say to a friend. I was curious whether the LLM would understand not just logistics, but vibe.\n\n\nExperiment: Prompting LLMs Like a Human Friend\nTo test GPT-4’s ability to offer travel ideas, I gave it 3 kinds of prompts:\n\nMood-based: “I need to clear my head.”\nPersonality-based: “I’m not a beach person. I like bookstores and wine.”\nBudget-based: “I’m broke but I need a change of scenery.”\n\n\n\n\nResults and Observations\n\nPrompt 1: “I need to clear my head.”\nResponse:\n&gt; “For a peaceful reset, you might enjoy a retreat in Sedona, Arizona with its red rock formations and quiet trails. Or consider a solo trip to Iceland, where the landscapes are dramatic and the population is low. Both offer space to reflect and disconnect.”\nAnalysis:\nThe model understood this as a need for solitude and recommended locations that are scenic but quiet. It didn’t just list popular cities, which shows some emotional inference. This answer felt human.\n\n\n\nPrompt 2: “I’m not a beach person. I like bookstores and wine.”\nResponse:\n&gt; “You might enjoy a literary escape in Edinburgh, Scotland — known for its historic bookstores, cozy cafes, and whisky tastings. Alternatively, Porto, Portugal offers beautiful libraries and wine tours along the Douro River.”\nAnalysis:\nThis response showed impressive taste-matching. It avoided beach destinations and matched aesthetic preferences. The answer felt curated, even though the input was short.\n\n\n\nPrompt 3: “I’m broke but I need a change of scenery.”\nResponse:\n&gt; “Consider local travel options like taking a train to a nearby town for a weekend getaway. Many cities also have budget hostels and free walking tours. If you’re open to volunteering, platforms like Workaway let you exchange work for lodging around the world.”\nAnalysis:\nThe model understood the tone — a mix of frustration and desire — and offered ideas that felt realistic and supportive. It didn’t suggest luxury hotels, which shows context awareness.\n\n\n\n\nConclusion\nLLMs can be surprisingly good travel companions — not just for logistics, but for tone, budget, and mood. While they don’t know your full life story, they’re able to reflect back what you express and turn it into thoughtful suggestions. That said, the best results come when you treat the model more like a friend than a search engine.\nAs LLMs improve, they may shape how we dream, plan, and reflect on travel — not just where we go, but why we go there."
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/009_Ninth_Post/index09.html#introduction",
    "href": "posts/009_Ninth_Post/index09.html#introduction",
    "title": "Testing LLMs: Can They Identify Aesthetics?",
    "section": "Introduction",
    "text": "Introduction\nFrom Pinterest boards to TikTok trends, aesthetics are everywhere. They go beyond outfits and include mood, color, music, and lifestyle. But can a language model really understand what makes something feel cottagecore or Y2K? I wanted to see if GPT4 could identify online aesthetics and describe them in a way that feels true to how people use them."
  },
  {
    "objectID": "posts/009_Ninth_Post/index09.html#experiment-asking-about-aesthetics",
    "href": "posts/009_Ninth_Post/index09.html#experiment-asking-about-aesthetics",
    "title": "Testing LLMs: Can They Identify Aesthetics?",
    "section": "Experiment: Asking About Aesthetics",
    "text": "Experiment: Asking About Aesthetics\nI gave the model three different prompts based on popular internet aesthetics. I asked it to describe the aesthetic and give examples of outfits, colors, or vibes that match.\n“I want to try the clean girl aesthetic. What does that look like”\n“What is the difference between dark academia and light academia”\n“Describe the grunge aesthetic but make it sound poetic”"
  },
  {
    "objectID": "posts/009_Ninth_Post/index09.html#prompt-1-i-want-to-try-the-clean-girl-aesthetic.-what-does-that-look-like",
    "href": "posts/009_Ninth_Post/index09.html#prompt-1-i-want-to-try-the-clean-girl-aesthetic.-what-does-that-look-like",
    "title": "Testing LLMs: Can They Identify Aesthetics?",
    "section": "Prompt 1: “I want to try the clean girl aesthetic. What does that look like”",
    "text": "Prompt 1: “I want to try the clean girl aesthetic. What does that look like”\nResponse:\nThe clean girl aesthetic is all about minimal effort that looks intentional. Think slicked back hair, gold hoops, dewy skin, and neutral tones. Outfits are simple but put together, like a white tank with wide leg trousers and sneakers. The vibe is calm, confident, and organized.\nAnalysis:\nThis was a strong answer. It matched the tone of how influencers and users describe the clean girl look. It avoided over explaining and offered specific details that someone could actually use. It also felt current."
  },
  {
    "objectID": "posts/009_Ninth_Post/index09.html#prompt-2-what-is-the-difference-between-dark-academia-and-light-academia",
    "href": "posts/009_Ninth_Post/index09.html#prompt-2-what-is-the-difference-between-dark-academia-and-light-academia",
    "title": "Testing LLMs: Can They Identify Aesthetics?",
    "section": "Prompt 2: “What is the difference between dark academia and light academia”",
    "text": "Prompt 2: “What is the difference between dark academia and light academia”\nResponse:\nDark academia is moody, intellectual, and nostalgic. Think of brown coats, candlelit libraries, and rainy weather. Light academia is softer and more romantic. It still loves books but adds cream tones, sunny courtyards, and linen skirts. Both focus on learning and introspection but through different moods.\nAnalysis:\nThe model clearly understood the emotional side of both aesthetics. It gave more than just color palettes. The answer reflected the aesthetic experience, not just the clothes, which made it feel thoughtful."
  },
  {
    "objectID": "posts/009_Ninth_Post/index09.html#prompt-3-describe-the-grunge-aesthetic-but-make-it-sound-poetic",
    "href": "posts/009_Ninth_Post/index09.html#prompt-3-describe-the-grunge-aesthetic-but-make-it-sound-poetic",
    "title": "Testing LLMs: Can They Identify Aesthetics?",
    "section": "Prompt 3: “Describe the grunge aesthetic but make it sound poetic”",
    "text": "Prompt 3: “Describe the grunge aesthetic but make it sound poetic”\nResponse:\nGrunge is worn in and restless. It smells like basement shows and faded dreams. Flannel hangs loose over band tees and combat boots stomp over puddles of spilled beer. There is rebellion in every ripped seam and a kind of beauty in not caring too much.\nAnalysis:\nThis was surprisingly creative. It captured the tone and texture of the grunge aesthetic and made it feel alive. It went beyond basic traits and sounded like something you might hear in a Tumblr caption or a short story."
  },
  {
    "objectID": "posts/009_Ninth_Post/index09.html#conclusion",
    "href": "posts/009_Ninth_Post/index09.html#conclusion",
    "title": "Testing LLMs: Can They Identify Aesthetics?",
    "section": "Conclusion",
    "text": "Conclusion\nLLMs like GPT4 can identify online aesthetics with surprising depth. They recognize the emotional tone, cultural references, and even the lifestyle behind a trend. Whether you are asking for outfit ideas or just trying to find your vibe, the model gives responses that are both useful and expressive. While it cannot scroll Pinterest or try on clothes, it knows how people talk about style and can reflect that back with real clarity."
  },
  {
    "objectID": "posts/002_Second Post /index02.html#introduction",
    "href": "posts/002_Second Post /index02.html#introduction",
    "title": "Testing LLMs: Can They Recommend the Best Place to Shop?",
    "section": "Introduction",
    "text": "Introduction\nEveryone shops, but we don’t all shop the same way. Some people want designer boutiques. Others look for vintage finds, open-air markets, or budget deals. I wanted to test how well an LLM could help me pick the best place to shop — not just in general, but based on vibe, budget, and style. Could it tailor its answer like a friend would?\nInstead of asking for a list of malls, I gave GPT-4 more open-ended prompts, the way you might text a friend before a trip."
  },
  {
    "objectID": "posts/002_Second Post /index02.html#experiment-shopping-prompts-with-style",
    "href": "posts/002_Second Post /index02.html#experiment-shopping-prompts-with-style",
    "title": "Testing LLMs: Can They Recommend the Best Place to Shop?",
    "section": "Experiment: Shopping Prompts with Style",
    "text": "Experiment: Shopping Prompts with Style\nI tested three different prompts:\n\n“I want to shop like a rich tourist.”\n“I like markets, thrift stores, and handmade stuff.”\n“I need clothes but I’m broke and in a new city.”"
  },
  {
    "objectID": "posts/002_Second Post /index02.html#prompt-1-i-want-to-shop-like-a-rich-tourist.",
    "href": "posts/002_Second Post /index02.html#prompt-1-i-want-to-shop-like-a-rich-tourist.",
    "title": "Testing LLMs: Can They Recommend the Best Place to Shop?",
    "section": "Prompt 1: “I want to shop like a rich tourist.”",
    "text": "Prompt 1: “I want to shop like a rich tourist.”\nResponse:\n“If you’re looking for high-end shopping, consider the Ginza district in Tokyo, which offers luxury flagship stores and department stores like Mitsukoshi and Wako. Paris’s Avenue Montaigne and Dubai Mall are also top-tier destinations for designer shopping.”\nAnalysis:\nThe model responded with globally known luxury hubs. It understood the tone — aspirational and indulgent — and suggested locations that reflect wealth and exclusivity. It was helpful, but safe. A little more personalization (like hidden designer consignment spots) would make it feel more curated."
  },
  {
    "objectID": "posts/002_Second Post /index02.html#prompt-2-i-like-markets-thrift-stores-and-handmade-stuff.",
    "href": "posts/002_Second Post /index02.html#prompt-2-i-like-markets-thrift-stores-and-handmade-stuff.",
    "title": "Testing LLMs: Can They Recommend the Best Place to Shop?",
    "section": "Prompt 2: “I like markets, thrift stores, and handmade stuff.”",
    "text": "Prompt 2: “I like markets, thrift stores, and handmade stuff.”\nResponse:\n“You might enjoy shopping in Chiang Mai’s Sunday Walking Street Market in Thailand or browsing secondhand stores in Copenhagen’s Nørrebro district. For handmade crafts, Oaxaca in Mexico has a rich tradition of artisan goods including pottery, textiles, and jewelry.”\nAnalysis:\nThis answer had real personality. It listed places that fit a traveler looking for creativity and authenticity, not just price. GPT-4 picked up on aesthetic preferences and offered culturally relevant locations that weren’t on every travel blog’s front page."
  },
  {
    "objectID": "posts/002_Second Post /index02.html#prompt-3-i-need-clothes-but-im-broke-and-in-a-new-city.",
    "href": "posts/002_Second Post /index02.html#prompt-3-i-need-clothes-but-im-broke-and-in-a-new-city.",
    "title": "Testing LLMs: Can They Recommend the Best Place to Shop?",
    "section": "Prompt 3: “I need clothes but I’m broke and in a new city.”",
    "text": "Prompt 3: “I need clothes but I’m broke and in a new city.”\nResponse:\n“Look for chain thrift stores like Goodwill or Value Village if you’re in the US or Canada. In many cities, local Facebook groups or apps like Vinted and Depop are popular for secondhand fashion. Some student areas have budget clothing markets or swap events.”\nAnalysis:\nThis answer was practical and showed awareness of how real people solve everyday problems. It didn’t assume the user was traveling and gave ideas that apply locally and online. It also subtly accounted for cost, context, and effort — a sign that GPT-4 can interpret tone well."
  },
  {
    "objectID": "posts/002_Second Post /index02.html#conclusion",
    "href": "posts/002_Second Post /index02.html#conclusion",
    "title": "Testing LLMs: Can They Recommend the Best Place to Shop?",
    "section": "Conclusion",
    "text": "Conclusion\nWhen it comes to shopping, GPT-4 can do more than suggest a famous mall. It tailors advice based on tone, budget, and personality — if the prompt is clear. While it lacks the personal stories or insider tips a friend might share, its suggestions feel thoughtful and globally aware. For planning your next retail adventure, it’s not a bad companion."
  },
  {
    "objectID": "posts/005_Fifth_Post/index05.html#introduction",
    "href": "posts/005_Fifth_Post/index05.html#introduction",
    "title": "Testing LLMs: Can LLMs Adapt to Different Writing Styles?",
    "section": "Introduction",
    "text": "Introduction\n\nWriting is more than just putting words together—it involves tone, structure, and nuance. Whether it’s formal business writing, casual social media posts, or poetic storytelling, the ability to switch styles is a skill that separates good writers from great ones. But what about AI? Can Large Language Models (LLMs) effectively adapt to different writing styles? Let’s find out.\n\n\nExperiment: Testing Style Adaptation\nTo evaluate how well an LLM can switch between writing styles, I tested it in three areas:\n\nProfessional vs. Casual Tone: Asking the model to rewrite the same content in both formal and informal styles.\nCreative Writing: Providing a simple idea and seeing how well the LLM could turn it into a compelling short story.\nSocial Media vs. Academic Writing: Comparing how the model frames the same information for different audiences.\n### Results and Observations\n#### Professional vs. Casual Tone\nThe LLM did well in adjusting tone, though some formal responses felt a bit too robotic.\nExample:\n\nPrompt: “Write a formal email apologizing for missing a meeting.”\nResponse: “Dear [Recipient], I sincerely apologize for missing our scheduled meeting today. An unforeseen circumstance arose, and I regret any inconvenience caused. Please let me know a suitable time to reschedule. Best regards, [Your Name].”\nAnalysis: The response is polished but could use a touch of warmth to feel more natural.\nPrompt: “Now, rewrite that in a casual tone.”\nResponse: “Hey [Recipient], Sorry for missing our meeting earlier! Something unexpected came up, but I’d love to reschedule if you’re available. Let me know what works for you. Thanks!”\nAnalysis: The informal version reads much more naturally, showing that the model can successfully adjust tone.\n\n#### Creative Writing\nWhen asked to write creatively, the LLM generated well-structured narratives but sometimes lacked originality.\nExample:\n\nPrompt: “Write a short suspenseful story about a lost key.”\nResponse: “Emily’s heart pounded as she rummaged through her bag. The key to the old cabin—her only refuge—was gone. Footsteps crunched outside, growing closer. Her breath caught as she turned… and saw the door was already ajar.”\nAnalysis: The structure is solid, but the suspense feels somewhat formulaic. The model does well with classic storytelling but struggles with unpredictability.\n\n#### Social Media vs. Academic Writing\nThe model effectively adjusted to different audiences but sometimes overcompensated, making academic writing too dense.\nExample:\n\nPrompt: “Explain climate change for Twitter.”\nResponse: “Earth’s heating up, and we’re the cause. Burning fossil fuels = more CO2 = higher temps. Solutions? Cut emissions & go green. #ClimateAction”\nAnalysis: The tweet is concise and engaging, a good fit for social media.\nPrompt: “Now, explain it for a research paper.”\nResponse: “Climate change refers to long-term shifts in temperature and weather patterns, primarily due to anthropogenic greenhouse gas emissions. The combustion of fossil fuels has significantly increased atmospheric CO2 levels, leading to rising global temperatures and environmental disruptions. Mitigation strategies must focus on reducing emissions and transitioning to sustainable energy sources.”\nAnalysis: The response is informative but could be more readable."
  },
  {
    "objectID": "posts/008_Eighth_Post/index08.html#introduction",
    "href": "posts/008_Eighth_Post/index08.html#introduction",
    "title": "Testing LLMs: Can They Help You Dress for the Weather?",
    "section": "Introduction",
    "text": "Introduction\nChoosing what to wear is something we do every day, but sometimes it is hard to match the weather, the vibe, and your plans. I wanted to see if GPT4 could help with that. Could it understand the temperature, forecast, and even the mood of a day and recommend something useful to wear?\nTo test this, I gave it a few casual prompts about what to wear in different places and conditions. I looked at whether the suggestions were practical, accurate, and felt like something I would actually wear."
  },
  {
    "objectID": "posts/008_Eighth_Post/index08.html#experiment-prompting-gpt4-about-outfits-and-weather",
    "href": "posts/008_Eighth_Post/index08.html#experiment-prompting-gpt4-about-outfits-and-weather",
    "title": "Testing LLMs: Can They Help You Dress for the Weather?",
    "section": "Experiment: Prompting GPT4 About Outfits and Weather",
    "text": "Experiment: Prompting GPT4 About Outfits and Weather\nI gave the model three different prompts\n“I want to know what to wear in Seoul in April”\n“It is raining and I have a first date. Help”\n“It is 90 degrees but I still want to look put together”"
  },
  {
    "objectID": "posts/008_Eighth_Post/index08.html#prompt-1-what-should-i-wear-in-seoul-in-april",
    "href": "posts/008_Eighth_Post/index08.html#prompt-1-what-should-i-wear-in-seoul-in-april",
    "title": "Testing LLMs: Can They Help You Dress for the Weather?",
    "section": "Prompt 1: “What should I wear in Seoul in April”",
    "text": "Prompt 1: “What should I wear in Seoul in April”\nResponse:\nApril in Seoul is usually mild with temperatures between 50 and 65 degrees Fahrenheit. A light jacket or cardigan over a long sleeve shirt and jeans would work well. Bring a small umbrella just in case since spring showers are common.\nAnalysis:\nThe response was weather accurate and culturally aware. It suggested layers which makes sense for transitional spring weather. It also reminded me to bring an umbrella without overdoing it. This answer felt thoughtful."
  },
  {
    "objectID": "posts/008_Eighth_Post/index08.html#prompt-2-it-is-raining-and-i-have-a-first-date.-help",
    "href": "posts/008_Eighth_Post/index08.html#prompt-2-it-is-raining-and-i-have-a-first-date.-help",
    "title": "Testing LLMs: Can They Help You Dress for the Weather?",
    "section": "Prompt 2: “It is raining and I have a first date. Help”",
    "text": "Prompt 2: “It is raining and I have a first date. Help”\nResponse:\nGo with waterproof boots or sneakers that still look clean. Pair them with dark jeans and a nice top or lightweight sweater. Add a sleek raincoat or trench that will keep you dry but still look stylish. Carry a compact umbrella to stay hands free.\nAnalysis:\nThe model understood the stakes of the situation. It gave a mix of practical and stylish options and did not suggest anything unrealistic. It also felt gender neutral and flexible which made the advice feel more inclusive."
  },
  {
    "objectID": "posts/008_Eighth_Post/index08.html#prompt-3-it-is-90-degrees-but-i-still-want-to-look-put-together",
    "href": "posts/008_Eighth_Post/index08.html#prompt-3-it-is-90-degrees-but-i-still-want-to-look-put-together",
    "title": "Testing LLMs: Can They Help You Dress for the Weather?",
    "section": "Prompt 3: “It is 90 degrees but I still want to look put together”",
    "text": "Prompt 3: “It is 90 degrees but I still want to look put together”\nResponse:\nTry a breezy linen or cotton outfit like a short sleeve button down with tailored shorts or a midi dress. Go for light colors to reflect heat. Keep accessories minimal and use sunscreen. A pair of white sneakers or sandals can complete the look.\nAnalysis:\nThe outfit ideas were realistic and fashion forward without trying too hard. It gave both clothing advice and helpful tips for staying cool and protected. The response balanced comfort with style well."
  },
  {
    "objectID": "posts/008_Eighth_Post/index08.html#conclusion",
    "href": "posts/008_Eighth_Post/index08.html#conclusion",
    "title": "Testing LLMs: Can They Help You Dress for the Weather?",
    "section": "Conclusion",
    "text": "Conclusion\nGPT4 can be surprisingly helpful when it comes to outfit choices and weather planning. Its suggestions are practical, flexible, and often show good taste. While it is not a personal stylist, it can definitely help you avoid showing up to class soaked or wearing boots in the heat. If you are ever standing in front of your closet and not sure what works for the weather, asking an LLM might save you time and stress."
  },
  {
    "objectID": "posts/007_Seventh_Post/index07.html#introduction",
    "href": "posts/007_Seventh_Post/index07.html#introduction",
    "title": "Testing LLMs: Can They Recommend the Right Song?",
    "section": "Introduction",
    "text": "Introduction\nMusic is deeply personal. We turn to it when we’re sad, hyped, nostalgic, or trying to focus. So I wondered — can a language model recommend a song that actually fits the moment? Not just something popular, but something that feels right.\nTo find out, I gave GPT4 some music related prompts that were based on emotions or settings. I wanted to see if it could go beyond charts and suggest songs that match a vibe."
  },
  {
    "objectID": "posts/007_Seventh_Post/index07.html#experiment-prompting-gpt4-for-songs",
    "href": "posts/007_Seventh_Post/index07.html#experiment-prompting-gpt4-for-songs",
    "title": "Testing LLMs: Can They Recommend the Right Song?",
    "section": "Experiment: Prompting GPT4 for Songs",
    "text": "Experiment: Prompting GPT4 for Songs\nI gave the model three different prompts\n\n“I’m feeling homesick but trying to stay hopeful”\n“Give me a song to walk confidently into a job interview”\n“Recommend a track for a late night solo drive”"
  },
  {
    "objectID": "posts/007_Seventh_Post/index07.html#prompt-1-im-feeling-homesick-but-trying-to-stay-hopeful",
    "href": "posts/007_Seventh_Post/index07.html#prompt-1-im-feeling-homesick-but-trying-to-stay-hopeful",
    "title": "Testing LLMs: Can They Recommend the Right Song?",
    "section": "Prompt 1: “I’m feeling homesick but trying to stay hopeful”",
    "text": "Prompt 1: “I’m feeling homesick but trying to stay hopeful”\nResponse:\nYou might like “Home” by Edward Sharpe and The Magnetic Zeros. It’s about finding comfort wherever your people are. Another option is “Coming Home” by Leon Bridges, which blends longing and warmth in a classic soul style.\nAnalysis:\nThis response struck a good emotional balance. It acknowledged the sadness without leaning into it too hard. The songs were reflective but not depressing, which was exactly what the prompt called for."
  },
  {
    "objectID": "posts/007_Seventh_Post/index07.html#prompt-2-give-me-a-song-to-walk-confidently-into-a-job-interview",
    "href": "posts/007_Seventh_Post/index07.html#prompt-2-give-me-a-song-to-walk-confidently-into-a-job-interview",
    "title": "Testing LLMs: Can They Recommend the Right Song?",
    "section": "Prompt 2: “Give me a song to walk confidently into a job interview”",
    "text": "Prompt 2: “Give me a song to walk confidently into a job interview”\nResponse:\nTry “Stronger” by Kanye West or “Run the World (Girls)” by Beyoncé. Both are high energy tracks that boost confidence and mindset.\nAnalysis:\nThe recommendations were mainstream but on point. The model clearly understood the goal. A more tailored suggestion might have impressed me more, but these would work for most people."
  },
  {
    "objectID": "posts/007_Seventh_Post/index07.html#prompt-3-recommend-a-track-for-a-late-night-solo-drive",
    "href": "posts/007_Seventh_Post/index07.html#prompt-3-recommend-a-track-for-a-late-night-solo-drive",
    "title": "Testing LLMs: Can They Recommend the Right Song?",
    "section": "Prompt 3: “Recommend a track for a late night solo drive”",
    "text": "Prompt 3: “Recommend a track for a late night solo drive”\nResponse:\n“Nightcall” by Kavinsky is a great pick. It’s moody and has that slow driving feel. “Motion Picture Soundtrack” by Radiohead is another option if you’re feeling introspective.\nAnalysis:\nThis was the most impressive response. It understood the mood completely. The suggestions weren’t just songs about driving. They matched the feeling of being alone with your thoughts on the road."
  },
  {
    "objectID": "posts/007_Seventh_Post/index07.html#conclusion",
    "href": "posts/007_Seventh_Post/index07.html#conclusion",
    "title": "Testing LLMs: Can They Recommend the Right Song?",
    "section": "Conclusion",
    "text": "Conclusion\nGPT4 can recommend songs that feel surprisingly on target, especially when you give it emotional or situational context. It won’t replace a friend who knows your exact taste, but it’s more than a random playlist generator. If you treat the prompt like a conversation, it gives answers that make sense. So next time you don’t know what to listen to, asking an LLM might be worth a try."
  },
  {
    "objectID": "posts/006_Sixth_Post/index06.html#introduction",
    "href": "posts/006_Sixth_Post/index06.html#introduction",
    "title": "Testing LLMs: Can LLMs Handle Ethical Dilemmas?",
    "section": "Introduction",
    "text": "Introduction\nEthical dilemmas are some of the most complex challenges in decision-making. They require balancing moral principles, weighing potential consequences, and often navigating gray areas where there isn’t a clear right or wrong answer. While Large Language Models (LLMs) excel at processing information and generating responses, can they truly engage with ethical reasoning? Let’s explore how well they handle moral dilemmas.\n\nExperiment: Testing Ethical Decision-Making\nTo evaluate an LLM’s ability to process ethical dilemmas, I tested it in three ways:\n\nClassic Thought Experiments: Presenting well-known ethical problems like the trolley problem.\nContext-Based Ethics: Providing real-world moral dilemmas and analyzing the model’s reasoning.\nPersonal vs. Societal Ethics: Comparing individual moral choices against broader ethical frameworks.\n### Results and Observations\n#### Classic Thought Experiments\nThe LLM responded with structured reasoning but often avoided taking a definitive stance.\nExample:\n\nPrompt: “You see a runaway trolley heading toward five people on the tracks. You can pull a lever to divert it, but it will hit one person instead. What do you do?”\nResponse: “This is a well-known ethical dilemma. Some argue that pulling the lever is justified based on utilitarianism (minimizing harm), while others believe actively making a choice to harm someone is morally wrong. Ultimately, the decision depends on one’s ethical framework.”\nAnalysis: The response provides a fair overview but avoids a clear choice, likely due to the model’s programming to remain neutral.\n\n#### Context-Based Ethics\nWhen given real-world scenarios, the LLM offered thoughtful responses but lacked deep contextual awareness.\nExample:\n\nPrompt: “A friend confesses they cheated on an important exam. If you report them, they might lose their scholarship. What do you do?”\nResponse: “This situation presents a conflict between honesty and loyalty. Reporting them upholds academic integrity but could have serious consequences for your friend. Some may choose to encourage them to admit their mistake, while others might remain silent to protect their friend’s future.”\nAnalysis: The model outlines the dilemma well but doesn’t go beyond surface-level ethical reasoning.\n\n#### Personal vs. Societal Ethics\nThe LLM recognized differences between individual and societal moral frameworks but sometimes overgeneralized.\nExample:\n\nPrompt: “Is it ever acceptable to lie?”\nResponse: “Lying is generally considered unethical, but there are situations where it might be justified, such as lying to protect someone from harm. The acceptability of lying depends on the context and ethical perspective—utilitarianism might justify it if it prevents greater harm, while deontological ethics might reject it outright.”\nAnalysis: The model captures different perspectives but doesn’t demonstrate nuanced moral judgment.\n\n### Conclusion\nLLMs can engage with ethical dilemmas by outlining different perspectives and logical arguments. However, they struggle with providing nuanced moral reasoning and often default to neutrality. While useful for exploring ethical debates, their limitations highlight the importance of human judgment in complex decision-making."
  },
  {
    "objectID": "posts/003_Third_Post /index03.html#introduction",
    "href": "posts/003_Third_Post /index03.html#introduction",
    "title": "Testing LLMs: Can They Write Appropriate and Funny Jokes?",
    "section": "Introduction",
    "text": "Introduction\nHumor is tricky. What’s funny to one person might be awkward or offensive to another. Writing a good joke means knowing your audience, reading the room, and understanding timing. I wanted to see how well LLMs like GPT-4 could tell jokes that are both appropriate and actually funny. Could it make someone laugh without crossing a line?"
  },
  {
    "objectID": "posts/003_Third_Post /index03.html#experiment-prompting-for-jokes",
    "href": "posts/003_Third_Post /index03.html#experiment-prompting-for-jokes",
    "title": "Testing LLMs: Can They Write Appropriate and Funny Jokes?",
    "section": "Experiment: Prompting for Jokes",
    "text": "Experiment: Prompting for Jokes\nI tested the model with three kinds of prompts:\n\n“Tell me a joke a kid would laugh at.”\n“Tell me a clever joke that’s safe for work.”\n“Tell me a joke for a college student that isn’t cringey.”"
  },
  {
    "objectID": "posts/003_Third_Post /index03.html#prompt-1-tell-me-a-joke-a-kid-would-laugh-at.",
    "href": "posts/003_Third_Post /index03.html#prompt-1-tell-me-a-joke-a-kid-would-laugh-at.",
    "title": "Testing LLMs: Can They Write Appropriate and Funny Jokes?",
    "section": "Prompt 1: “Tell me a joke a kid would laugh at.”",
    "text": "Prompt 1: “Tell me a joke a kid would laugh at.”\nResponse:\n“What did one wall say to the other wall?\nI’ll meet you at the corner.”\nAnalysis:\nThis joke is simple, harmless, and plays with basic word logic. It’s the kind of thing you’d hear on a popsicle stick, which is probably the right level for kids. It didn’t try to be edgy or ironic, which was good."
  },
  {
    "objectID": "posts/003_Third_Post /index03.html#prompt-2-tell-me-a-clever-joke-thats-safe-for-work.",
    "href": "posts/003_Third_Post /index03.html#prompt-2-tell-me-a-clever-joke-thats-safe-for-work.",
    "title": "Testing LLMs: Can They Write Appropriate and Funny Jokes?",
    "section": "Prompt 2: “Tell me a clever joke that’s safe for work.”",
    "text": "Prompt 2: “Tell me a clever joke that’s safe for work.”\nResponse:\n“Why don’t scientists trust atoms?\nBecause they make up everything.”\nAnalysis:\nThis is a classic science pun, but it still works. It’s safe, clean, and shows a little cleverness. It’s also short, which helps with delivery. While not original, the model’s choice was on target for a workplace context."
  },
  {
    "objectID": "posts/003_Third_Post /index03.html#prompt-3-tell-me-a-joke-for-a-college-student-that-isnt-cringey.",
    "href": "posts/003_Third_Post /index03.html#prompt-3-tell-me-a-joke-for-a-college-student-that-isnt-cringey.",
    "title": "Testing LLMs: Can They Write Appropriate and Funny Jokes?",
    "section": "Prompt 3: “Tell me a joke for a college student that isn’t cringey.”",
    "text": "Prompt 3: “Tell me a joke for a college student that isn’t cringey.”\nResponse:\n“Why did the student eat their homework?\nBecause the professor said it was a piece of cake.”\nAnalysis:\nIt leans a little corny, but it still relates to student life. It didn’t try too hard to sound young or internet-y, which helped it avoid sounding forced. If anything, it felt more like something you’d hear in a student newspaper than on TikTok."
  },
  {
    "objectID": "posts/003_Third_Post /index03.html#conclusion",
    "href": "posts/003_Third_Post /index03.html#conclusion",
    "title": "Testing LLMs: Can They Write Appropriate and Funny Jokes?",
    "section": "Conclusion",
    "text": "Conclusion\nGPT-4 can write safe, audience-specific jokes — as long as you’re clear in your prompt. It knows what kind of humor fits each setting, and it avoids anything offensive. That said, it tends to lean on familiar material. The jokes are rarely original, but they’re rarely inappropriate either. So if you’re looking for light laughs that won’t get you in trouble, an LLM can deliver. Just don’t expect stand-up level comedy… yet."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hellen’s Blog!",
    "section": "",
    "text": "About this blog"
  }
]